everyone we'll get minutes. get started. everyone see slides. kazumi saying pause reason y'all see right right apologies issue the. recording apparently know happened Mondays lecture reason recording show the. slides face, thankfully, slides recorded issue dominant system classes, issue and. Dartmouth, looking fixing meantime posted link actual recording includes share slides well. made announcement yesterday, want review material yesterday Monday, please look recording sent Dartmouth fixes. problem super awkward looking like face two hours that's really fun look at, hope add slides back anyway so. Today we're going continue our. unsupervised learning. i'm going introduce add last two class. clustering algorithms called db scan htc. reminder homework due tomorrow midnight, please. submit time, homework released, right number four much smaller made small could within week. could have. remove homework form class, think good get practice unsupervised learning well, oh slides advancing you. advanced oh right, let me. going here, let try get right. long, long lecture even open lecture tonight. right, let try again. think zoomed last lecture that's happened, see really advanced good yeah homework due tomorrow and. homework released tomorrow well much shorter instead removing together. class good get experience unsupervised learning well actually quite fun. homework different homework doing, gradient descent anything like that, optimizing cost function before. one algorithmic writing know, probably noticed unsupervised learning algorithms really about. Learning sense supervised learning is. homework grades released, please take look them, let know issues going also wrap mid term grading early next week latest then. need rush great homework, three four next week, basically final exam, think it. right. obviously far, know summarize learning we've talking clustering, main. Part unsupervised learning lecture last two lectures i've focusing kind K means family class Jane i've kind divided different kinds flavors K means so. is. Differences calculate centroid cluster ran two examples, know, others, know went K means came in, medians two different ways calculate centroid algorithm for. clustering exactly averaging. use medium one mean two average, also talked different ways initialize update clusters, two standard came us, introduced kind fit. version Kenyans called bisecting K means tell K means hierarchical clustering algorithm kind put heading, ways, think as. Different way difference clusters initialized update right, initialize cluster one large cluster you. update divided two two two everything we've discussed far clustering been. Part K means family clustering really important like popular famous family clustering already lot variation within it, see. know turn hierarchical clustering algorithm using bisecting K means. that's we've looking far. went ways evaluate goodness clustering and. we've run Monday, two ways grant ground truth ground truth, truth or, view ground truth. two metrics ground truth truth Silhouette score squared error, Silhouette score. recall, going draw clusters here. X kind centroid clusters. Silhouette score score takes account two things one see major angle, cluster it's. good clusters tight cluster meaning close points other. that's one factor, considers that's cloud Silhouette score metric second factor, considers is. far closest cluster basically fast tight cluster far clauses cluster idea good clustering would make sure cluster tightly packed. that's metric, sort causes tied together far apart that's good good clustering two criteria. class Sir tightly packed really far away closest cluster. that's Silhouette score measures, far classes ways, tightly packed clusters went Monday, combine two get Silhouette score. that's squared error, obviously Silhouette need ground truth. know distances clusters, without actually ground truth another metric covered. squared errors, rougher measured Silhouette concerned first criteria, tight clusters summer squared error basically looking each. Point cluster basically actually point measuring distance centroid sent right class eight blocks to. square square distance sums points basically there's of. squared distances points cluster central right, one nice metric quick calculate and. powerful Silhouette score means take account tightness cluster right, look distance point to. essentially capturing tight cluster really hard disperse clusters Silhouette ways, know powerful takes account metrics. discussed two metrics measuring goodness clustering ground truth. One purity mathematical actual algorithm quite simple covered Monday, conceptually looking much idea clusters right purity cluster homogeneous clusters. Meaning contain mainly data type not, know types data data points ground truth right that's calculate purity clusters. Obviously, perfect clustering would one cluster would content one type data totally pure. The. mentioned Monday, ground truth practice actually usually small portion data manually annotated right, need lot label data long representative sample. good way good strategy collecting ground truth, would cluster sample maybe one person data something like way make sure sample data cluster. know make sure draft data set going large sample data points manually annotated right. manual annotation independent clustering, way, shouldn't, true of. whenever kind manual annotation tasks. biased annotator letting know hey, classes, took data points make sense that. biasing annotator saying yeah makes sense, show results clustering say data points say. example gave Monday images animals tell label animals see images tell them, days, pictures came cluster not, leading the. annotator could result biased annotation ground truth good good strategy sample data points cluster based dedicated purity Rand index, actually. similar accuracy classification based something akin confusion matrix created for. Classification, difference because, remember confusion matrix really defined binary classification, two classes classes cluster not, cluster basically right that's create. confusion matrix Duran index calculated. pairs points, look combinations point pairs points, they're same. Cluster that's that's akin positive positive classical they're place that's like negative right based could create confusion matrix. exactly macro mathematical formula calculating accuracy applied calculate Rand index confusion matrix right. is, remember accuracy, stay true true positive true negative divided basically every data point have. here, positive negative terms closing means two points belong cluster. annotator said level that's know true positive two points belong class hitters agree belong level would true negatives. around example Monday, go review lectures. Okay. already discussed, know, like family chemistry discuss different ways average calculating central data cluster right and. already went couple like, example, mean median obviously use mode, kind frequent value data set wanted to. there's added slide initially thought good think think way average. set data points mean obviously means standard thing usually taking mean media probably second common. mode, might heard about, basically taking frequent value data said, think way averaging could anything anything else come mind. good yeah that. Yes, nice exactly already discussed. one add different meats right standard mean use called actually arthritic mean actually look geometric mean look harmonic mean think mentioned harmonic mean right. yeah agree, remember F1 score, way, harmonic mean of. recall precision, recall. anyways remember funds Koreans harmonic mean recall precision, remember go back check out, but. know even mean right even mean exactly like take mean something even making assumption assumption standard darth medic mean, everyone, everyone is. familiar with, might, must heard means definitely would heard geometric mean high school. remember covered harmonic mean know there's ways average well and, obviously, know. stick standard arithmetic mean that's safe bet know, depending kind data working different kind taking different types mean might appropriate K K means right. anyone anyone remember briefly talked harmonic class scenarios, would make sense use harmonic me. briefly talked talked F1 score harmonic mean precision recall. notice, know even even look perfect class, know knowledge, yes diane ISIS right. so, does. semi private chat, rates, want average rates values rates, average rates makes us us harmonic rate itself, ways kind average right. average averages, example, usually would use something like harmonic mean know worry it, used live in, example, physics finance things like that, know depending your. domain, might right physics, work lot rates right, know know average different rates us harmonic me. Okay, good example taking trip keep changing speed trip order look average speed better use harmonic meet medic trip. know right. anyways wanted slide want know covered class two ways averaging right. know even mean different know definitions, depending always keep mind, really want notice, class, know always. aware data working understanding kind mathematical operation makes sense terms K means simple replacement replace averaging function everything algorithm exactly Okay, then. discussed also. choosing far came i'm wrapping kamins i'm quick review talked financialization methods discussed random city visualization powerful could choose seeds, know, give us. Better. make us make algorithm go faster give us better results faster convergence run one example, Monday actually went example psychic learn users, called caymans plus plus. are, mean use plus Plus, way, general, way works, know really important know details it's. one method, way works basically points first start random selection, pick central first see. way kameez works Dan assigns probability points pick course they're probably add one. probability based far central that's already picked that's basically think three maybe like probability of. know, know, maybe point three, seem similar maybe point like point one five actually act adult say Point two five each. Point seven five go point nine say one boys point five probability, really far away. Right, definitely size, probabilities points randomly still randomly picked randall big PICs based probabilities. And, means point, example, five times likely picked point appoint say point picked real updates problem description points picks that's came plus plus works. there's question go ahead. Oh i'm sorry yes right. universities, point two, five, thank you. I. Yes, yes, yes, want further, higher probability i'm sorry, yes, would say point two, five then. add reason wanna throw magical random video high probability. Distribution property values, get gist it, points divide pig higher probability, thank correcting yes, so. that's one way pick one like Max distance, example, right, pick point. Then, take Max two point maximum away, point that's maximum away combination average location two points, example, forth, keep that. reason wanted slide know interested bored summer find something fun. come new random second rationalization method simple. know relatively simple method try could come know, maybe method estimate better kamins plus plus obviously better, define somehow. could define terms convergence speed obviously good dependent defined terms of. good clusters, end, usually given enough time clustering algorithm run less, get similar results, look speed like fast take many iterations for. classroom converge, instance, could could tell us little bit fun side project right knows, maybe they're really smart method that. outperforms every method that's there, could publish right, know that's that's like really fun small. machine learning research project summer, really really bored anyways random civilization, could know kind need methods, could also thinking methods. then, finally, course, came ease family K means talked about, except bisecting K means where. name suggests, said K, right bisecting still caveat, choose K discussed two ways, one would based Silhouette score. one based on. summer squared errors, confusion office hours on. Monday, plots representing maybe went little quick Monday know representing is. know this, two clusters Green blue black degrade, axis, think value one point green cluster right. Like take one point green cluster and. X axis silhouettes graph point remember St Louis point catalyst point. cycle evangelization sorted make look Nice, sort started, highest Silhouette lowest lead there, actually kind get sense shape. Right nice thing visualizing like immediately remember larger Silhouette better, immediately compare areas see. clustering getting better results right. really care sum area plaster area, red line showing basically average lead score basically two areas combine divided total number points. showing average score clustering obviously higher Silhouette score better, look average lead score. two different clusters, red red years later point seven around know around point five, like eight something. pick one better classmates that's representation is. need average Silhouette, course, look total Silhouette right number points clustering right data set matter Anna go ahead. got visualization purposes. matters fill other, matters actual average see score, visualization maybe confusing helpful right basically filled area get nice. visualization know what's happening yeah. get actually discreetly data point make sense. anyways that's one way it, pick try different case pick one gives highest score method using other. metrics. measuring goodness cluster unsupervised metric measuring goodness cluster, sum squared errors one quite simple, you. Basically, look sum square distances points central cluster centroid right. And, apply different case idea called elbow method, want pick point elbow sort squared error low, large cave because. K, larger case, likely, overfitting conceptually make sense because, very. Extreme assign cluster point cluster going get perfect Silhouette score apparently square because. they're going super tight like going like black hole gonna singularity right going tight, also going be. yeah song squared like high tide going super tight are. that's know want pick Lord K gives lower squares. Okay, times think good know clustering already discussed central topic clusters. hear already know know average show could anything, depending goal is, many times good know classes simple. hear talk clustering radius cluster basically maximum distance central cluster to. point cluster diameter Max distance two points cluster. density class, number points divided volume cluster. i'm going show visualizations help slide i'm gonna introduce send already know, average four points right. average case Euclidean average that's centroid right Center radius line, distance centroid largest third point furthest away the. century radius density would actually treat like circle, radius calculate area higher dimensions, value cluster. basically number points divided volume cluster density good usually want dance classes right want low want. want volume small, wanted wanted populated lot points. good enough people talk density density cluster density based clustering another set methods clustering tries maximize density, example, good know density refers to. remember area 2d actually volume higher dimensions. finally, diameter the. distance 230 third point classroom distance called diameter cluster mathematical banner two times radius distance largest points. terminology questions technology pretty straightforward. Okay. Oh yeah there's questions go ahead. 35 other. basically look distance every pair pants class pick largest instance that's dynamic. obviously data going hugely radius going hugely affected outliers right point that's assigned go back to. radius somehow know cluster know kind tight. then, point reason assigned that's here, obviously that's going stretch cluster going hire. going affect everything going higher radius say central somebody going higher radius. going higher diameter going lower density, that. want want Laura want small radius small diameter high density right want clump together, want radius diameter, small, want dancing high. kind met measure is. heavily affected outliers, good, use metrics figuring guide better clusters actually fluff others not. Okay, hard costs i'm going that's that's technology, good i'm going take step back and. remind plot hardcore clustering went second K means, divisive method. i'm going give example agglomerated method hierarchical clustering super simple example i'm going ask actually come method of. agglomerated clustering would say, remember kilometer clustering works like given assign point data set cluster. Right every iteration iterative methods intuitive every question want figure clusters marriage. Right yeah question go ahead. Okay, give everyone couple seconds think, i'll call you. second, would good method get everyone think couple seconds hear from. Think it. know private chat me. yeah couple right. Think it, complicated, mean simple algorithm right, best way decide clusters marriage yeah think i'll everyone far right choose ship G answer go ahead. yeah exactly kind of. metric measuring close classes are. already know know, example, calculate centers clusters, can, example, see close central is, could pick causes clusters merged that's simple method right. something super sophisticated something like enough getting agglomerated. clustering running. think everyone who. sent message think guided right, slides slides PowerPoint slides previous. leg class i'm going use this. right, everyone see this. slideshow play faster so. slide talks exactly people chat mentioned, simple algorithm agglomerated clustering i'm glad everyone. came obviously here's input, know setup set points. given metric measuring distance clusters, assume could become a. hyper parameter passing right, start obviously assigning every point song cluster called single tone cluster go every point data set assign cluster right that's start. know. this, one hyper parameter given another stopping condition could combination stopping conditions could another set hyper parameters, could number of. iterations could solid thing think slide, way, talking generic hierarchical agglomerated clustering right, see ideas behind simple is. assign mascot last date know, iterative process iteration decide clusters marriage simply just. going every pair clusters set right decide calculating distance them. Based distance metric passed input, hyper parameter passed right, generic here. combine two clusters argument opposite art Max, means going get parent class bag the. smallest distance merge two clusters come completely new cluster. then, course, update set clusters, important, new set includes marriage cluster Union, include marriage cluster. also remove two clusters marriage right, take set clusters, remove two classes merge, add new marriage cluster repeat right so. basically means like say three points simple, calculated distance remember start point cluster, calculate distance point. mean make point, little bit further, calculus four points like this, example, say know distance, calculate distance pair points. pair crop clusters i'm sorry coins stage, point cluster right. so, two seem closest based distance metric whatever might law says Euclidean distance simple, decided merge two one cluster. one cluster, remove two running remove two that's add this. set clusters, three clusters right. then, three classes, thing one merged, one, probably used probably marriage they're closest other. again, remove clusters two. course there's one option here, made sure, see actually lead eventually getting one cluster top. nice thing algorithm completely generic based two things hyper parameters distance metric measure distance two clusters stopping condition go, collaborative clustering algorithm. question distance could could anything could distance centroid. could things know that's may generic fact think next slide yeah go possible examples is. know, assuming usually Euclidean distance right even dad could know. Change like use Manhattan distance something like that. know PowerPoint database assuming using Euclidean distance could still different metrics calculate distance two clusters. can, example, look central distances central is, mentioned, could could actually. go every point two classes tlc prime go every point C every point see prime example right look minimum distance two points two clusters right so, example. know if. are, one cluster see say and. see prime, literally define distance function minimum distance point C C prime case, guess, would distance. know, way actually ways, see. Whether borders close right. could opposite, look maximum distance other, right way, see. Whether know outliers really far away look average distance, similar look distance centroid right. gave this, way, i'm saying one good good bad i'm saying define distance generic. could, first decide want use Euclidean distance not, even decide could different grades measure distance two clusters, could decide. look average points kind similar look distance centroid could look at. Max distance two clusters right, case would two points, probably, look MID distance, depending task is, might want choose one versus other. algorithm change. clear fact know met generic lot options to. Define distance metric could something completely different like could, working clients see clustering words. something like could look semantic distance words know words something like right, even anything. geometric. Okay. questions. questions right good yeah saw easy copy Aguilar MIT acclimated clustering algorithm right actually this. algorithm pretty much general capture agglomerated algorithms usually distance function, is, really be. Clear really adapted task course always iterative algorithms always thinking stopping condition collection stopping conditions. case aggravated clustering run get top cluster, way stop Sarah go ahead. Absolutely so. Point slow, lot data, mostly writing time Maybe initialization. yeah exactly yeah exactly would around. little smaller square know distance bna basically order n squared exactly so. initialization yeah exactly that's really good question yeah actually. means general algorithm right, means probably also think smarter way initialize your. Your. singleton want do, true hardcore clustering start leaf nodes each. Point zone class this, mean clustering expensive right whole point go point cluster one cluster. captures everything cases want maybe want start somewhere mid mid level, know. maybe better way initialize instead starting single tones maybe start know clusters, know, maybe you, know run. K means data right, say K hundred get 100 classes, use initialize hardcore clustering, example, right, mean think different ways initialize thereby faster take account whatever. task, know, something know speed important think ways initialize much faster basic algorithm. true hardcore clustering go point cluster one cluster catches everything. there's avoiding slow algorithm hardcore classes outside and. way, could said make could could make could use K means initialize right wanted to, build top that. Okay, journal article yeah generic algorithm think there's example here, laura's apologize that, say have. Data two dimensions remember apologize notation here, actually like X zero X. One unsupervised learning, know, think. professors harmed standard grid X, grid show but, basically, two features coin two features excellent next one. here, plotted data points data point two values excellent next one, corresponding futures. way hierarchical clustering course works know yeah points say working Euclidean space, know two points close other, look average distance centuries. points closest going calculate new centers assign cluster two points, know. there's way calculate centroid next time decide marriage cluster look century class, longer two months right. calculate new centroid signed two close us assign together then, finally, we're going stop there, know merging. Points and, way, point emerged longer running cluster longer going merged running clusters. Right we're going deciding three purple clusters single term clusters. then, next step actually closer's cluster actually closest best move actually add point. make point cluster consume close compared points, reason example show that. know case going merging points first going merging clusters greater like based distance right here. front, best movie actually leave single turns emerged branch. again, now, running going deciding probably next move would actually include maybe marriages, but. hard costing works whole thing one giant cluster. Oh actually best move next Western hostess Eric so. brings final. clustering algorithm i'm going talk class DVD scan. i'm going continue using professors slides well. The. db scan slide points actually probably widely used algorithm K means K means common one, pretty comfortable with. guy second common one quite fast, advantage K means, know need know number classes hardcore clustering algorithm normal clustering algorithm like amy's except not. need know number clusters advance. know setup exactly choose know data set want be, want class frank, reason. PowerPoint keeps hiding cursor anyways. know need know number clusters famous clustering algorithm came really important cover class. know there's saying know there's saying thing free lunch so. know, thing free lunch, meaning that, yes, need know number classes, but. must hyper parameters say, way, clustering algorithm good, hyper parameters set. so, yes, need know number classes, hyper parameters set hyper parameters actually designed bit more. intuitive ways saying want 10 clusters. scan hyper parameters. One epsilon is, greater zero denotes radius circle get used main points know, number points neighborhood we're going discuss. used actually discuss next PowerPoint next bullet point so. given give scan two parameters use Paris calculate every point, going every point data, maybe the. point data set can't get known neighborhood point neighborhood point defined go every point data set, point is. distance greater smaller equal epsilon said there's got neighborhood point right, understand this. epsilon capturing basically definition neighborhood close opponents include neighborhood. know. neighborhood particular point set points within sphere epsilon Center that. river radius epsilon Center point right that's epsilon used calculate neighborhood every point. again, like, said parameters db scan parameters they're supposed intuitive kind instead saying. specify number classes, explain kind dispersed want clusters, neighborhood absalon kind captures size neighborhood see us. Many skews clustering larger neighborhoods would correspond fewer clusters, dispersed clusters smaller neighborhoods correspond smaller clusters, tightly. tightly packed clusters. Alright, go examples here's data set say talked mean points used ignore say said epsilon too, means point neighborhood is. idea include either point.  epsilon one would neighborhood point say looking click Euclidean distance here. Well, nothing right there's point one point away. that's one man, one distance unit away neighborhood idea points, except point right on, forth okay. definition neighborhood four points right specified epsilon passing hyper parameter using calculated neighborhoods every point decide label points assignment three levels, two points we. assign assign point either core point border point noise point so. again, remember go points data can't get neighborhood points, decide point core. If, minimum points parameter comes play, neighborhood greater equal number points. it, specified main points right. know want neighborhood least mean points points it. hyper parameters nice make intuitive sense, epsilon size neighborhood obviously main point dense needed call cluster right so. Generally speaking want. Generally speaking generally want dense clusters. tied pack, want minimum points high right want like lot points classes and. want tightly packed then, course, depending test, might want play around lot points data, maybe might know main points much smaller right. Similarly, neighborhood maybe working. data working density matters lot that. size neighborhood something maybe want have. hotter larger neighborhood also larger minimum points right trade play around supposed take place, like take. replace K K means, supposed become intuitive so, two parameters neighborhood size neighborhood population basically our. neighborhood size maybe population intuitively tune, clustering. Okay, assign point core point satisfies condition number least many points set up, mean points neighborhood otherwise core point. either border point noise point border point obviously core point, satisfies condition transact enough points neighborhoods, included neighborhood core point. Right. becomes border point, otherwise becomes noise point. go example. So. parameters set right obviously know start point, point core point well, look epsilon three epsilon three would 12123 123 basically. 123 anything within square. we'd part neighborhood point, see that. obviously there's points here, three points, counting point fall neighborhood. disappointing given epsilon three said, important points three, satisfied, becomes core point least three points neighborhood, quarter point. fact, points, see they're within square right, neighborhood also includes three solid days four points four points right least three points neighborhood. yeah core points oh i'm sorry, coconuts Apparently, one core point oh changed say, see this. epsilon three, four points, then, sent change epsilon know, obviously changes, get still four points still have. three points within their. neighborhood point becomes border because, despite epsilon away from. Okay, points so, way, border point because, even though satisfy condition bigger corporate still included neighborhood two core clients. so. assigned border point corporate neighborhood core point becomes border point, whereas points. Obviously they're core points minimum number neighbors required, they're also neighborhood poor point they're labeled noise points. already see nice, ways noise captures outliers right. noise points good way figure points apply points four points they're, even neighborhood color points. Okay, three definitions basic idea behind db scan quite simple, is. start asked point going one three levels core border noise. start removing noise points basically outliers start essence set points start removing noise points. Then, whatever left, noise going core border right db scan actually create graph conceptually nice thinking graph. here's think conceptually db scan works the. Noise points removed say say have. left say. say know, noise point point know, two core points know, maybe three border points. first remove left Korean border points, then. way discount works, think graph notes whatever left Korean border points start basically graph nothing connected anything three border points. call warrior 101. two core points decide connect nodes graph first start core points. core points close neighborhood distance epsilon away remember. previous example showed core points top right we're within neighborhood right, corporates together they're actually part cluster right. first go core points within neighborhood within distance epsilon feature connect them. go needed core points they're excellent teacher connect them. said, say another call point like see know within FC zero se connect go border points decides. assign one clusters cluster is. connected component car points connected core points single core components nap correspond clusters right deciding. assign border points clusters, based core points right, horrifies assign. powerpoints mind clusters corporates close basically merging cluster corporates emerged. know many classes, have, case decide class assign border, point two, that, based distance so. assign border point cluster has. less epsilon away, know there's going cluster. like that, whole definition border point point belongs neighborhood core point there's going least one 4.4 which. border point atmos epsilon away right, know going least one cluster join sometimes one console pick randomly say. Border point zero caused us cluster gets assigned cluster unless get society classes that's works basically. Right simple, db scan clusters defined core points first thing identify core classes marriage, core points within epsilon other. left clusters assigned order points based criteria, nice because. Again, kind hyper parameters invite intuitive secondly good removing outliers noise clustering. go example this. script, labeled points and. say, parameters, pass db scan say want neighborhood least three points, want ability too. first thing db scan remember calculate core neighborhoods point go point. B, C E, F G, way pee neighbor hood point respect excellent defiant right neighborhoods points i'm saying neighbors point. first. Then. got points, one one, start say. going marked noise point because. know, one many points three right, doesn't. matter satisfy criteria core borderline marked noise. go marked noise satisfy CD Dr Martha noise, satisfy requirement. move F F actually noise point. has, see, actually way required number points neighborhood actually core point. Right, set aside classic number. clusters defined core points right saying A, B, C Eva noise ignore remove they're longer part class saying F. core point going define cluster might corporations neighborhood they're going merged class, F is. cluster want see points region added to. cluster, what's called expanding cluster right look neighbors F decide so. go know realize Oh, also core point with. different neighborhood actually has. F F points G H, its. neighbors. Oh, neighbor point F neighbor right so. extended now, added cluster well, know keep core point look points this. neighborhood expand or, again, know points neighborhood I, points neighborhood this. know intuitively know keep expanding keep expanding keep expanding longer expand, imagine, F. go all, expand go IMA expand I, imagine that, eventually, keep repeating eventually actually follow logic going going end points assigned cluster. repeat go next point, then, repeat repeat keep track points visit come back what's, point already assigned. cluster assigned noise, ignore Dana that's juice stuff. db scan said, relatively straightforward. algorithm relies identifying three types points doors identified using hyper parameters epsilon defines neighborhood point and. size neighborhood point main points defines population neighborhood we. remove noise points data set expand use core points way start classes expand clusters longer expand move next core point, on, far, questions there's one question. noise point noise point because. remember requirement never would three requirement be. core point core point within neighborhood epsilon equals two would would would be. he'll epsilon square neighborhood right many neighbors one. satisfy main points requirement core point border point well, would water pipe belongs to. neighborhood core point right, find close B b's a. core point calculations, nice neighborhood these, qualified also belong the. neighborhood cut second. Okay, video froze second um yeah options corporate also belong neighborhoods corporates that's definition noise point. think core points that's kind maybe sons border points planets orbiting noise points random asteroids shooting right so. really they're orbit corporates, get removed noise that's that's discount works, see actually eliminates lot points example. So, actually, neighborhood calculated initial pre tax neighborhoods every time, already see going noise, know kind infer gotta noise. think. yeah already already showed going noise going noise. CD us going noise, corporates also nine neighborhoods core point, so, fact believe know part part parts clusters, going form. pretty brutal removes lot points noise could remove so. are, think removing many outliers noise points could know either reduce number minimum clients require be. So, smaller that, then, example, remove set minimum points one amb longer be. Noise points fact they'll core points they're other. within neighborhood satisfied minimum requirement here, could expand size classes literally area clusters by. Increasing know similar effects this, they. reduce number fun throw surprise, two different things one. still keeping classes, reduce main points keep epsilon still keeping size classes, less making less dense less points right, whereas one know. expand size of. cluster like geometrically would actually larger area space. know, even though effect reducing number nodes points, two different things answer question. knew would noise point. Okay, questions um little bit a. with, hundred clustering algorithm iterative algorithm go every node everything least nice is, that. probably guess anyone guess running time algorithm. stop. stop visit notes right, notes right, list vc visited notes throw noise stop means actually quite fast linear. running time right visited nodes either throwing noise identified core points border points know whatnot actually. yeah semi right exactly so. actually quite fast admins much faster K means Another advantage db scan compared algorithms run linear time tuna go ahead. first step to. first step calculate neighborhood five point. Right. first thing do, db scan every point measure calculate neighborhood. executives implementation wise, would mean Python probably best way dictionary point neighbors right and. yeah that, first, course, yeah, otherwise able know, that, could actually function says. know assign labeled whatever point then, go, example, pass point function function going basically follow criteria. criteria going say right, already neighborhoods right already minimum points epsilon given hyper progress so. function, since already neighborhood see follows basically statement else. thing set up, state core point else Elif else know thing said, think otherwise noise make sense. um fast um robust outliers it's. um. less intuitive hyper parameters, might think, mean think intuitive people might think number clusters intuitive number clusters that's kind of. know already request much knowledge task know, like know there's like type 10 kinds data. want 10 clusters right, course ways choose K Similarly, way could tune parameters right exact way use Silhouette tune K. pick best guy could use Silhouette grid search points right, could have. Like range minimum points like one 10 excellence one 10 agreed 10 10 could pick one gives best Lewis course could tune NK it. ways, feel bit intuitive telling tell something size classes depopulation clusters. questions this. Okay, uh curious this, a. restraint Wikipedia, actually algorithm db scan. code exactly discussed right and. Nice, know. code nicely, know divided different functions whatnot know actual code lines look pseudo code new Wikipedia page db scan right. yeah, basically right so. believe implementing devious homework definitely something K means maybe db scan forgot, look that. think removed wanted make little bit shorter you, might add, bonus point llc actual algorithm quite straightforward Okay, actually kind of. covers everything wanted cover clustering. know, went several different types clustering algorithms and. discussed matrix of. megan goodness class rank also. introduced two types classes, know hierarchical kind standard clustering hierarchical clustering discussed two different methods classes top bottom up. K means top down, start large class divide second gaming's could dividing came class, came simple algorithm hardcore acclimated clustering again. relies distance function pass that's that's really tune our. data set probably noticed unsupervised learning much simpler supervised learning algorithmic. mathematical enough supervised learning cars funky know trying optimize based optimization problem here, something trying optimize cost less algorithmic. Part machine learning, true unsupervised learning, way true clustering. unsupervised learning methods algorithmic mathematical ones we've discussed class, usually deep learning based probably likely creating declining class. questions clustering move on. Okay, i'm gonna go back slides now. Okay, so. Okay, everyone see again. yeah. Good so. car class saying today, wanted spend rest day today talk something briefly discussed beginning class think important concept dig little bit deeper, idea feature engineering. you. know. abstract way know do. supervise also unsupervised learning say abstract mean give basically futures working learn models that, really important actually. know come features briefly talked important concept idea behind generate, part amount human intervention. plus hyper parameter tuning, know rest usually done automatically learning parameters whatnot but, futures futures using meaningless, then, nothing going work. so. futures informative models never coverage classes would meaningless, is. important human beings really interactive components machine learning height entertaining also defining future want models. work with, many different know techniques fall future engineering, discussed class earlier, is. raw data passing supervisor unsupervised model right, get future space, know, then, future space, can. use discussing class future engineering includes things like. would maybe fall data cleaning includes things like normalizing data data like huge know. range maybe want normalize smaller range know data might noisy might lot missing values, ways deal know histogram data know, instead having. know hundred categories, maybe 10 categories, understandable, kind of. manipulation data, use models discussed class kind put feature engineering, even though people call kind of. like dealing missing values normalization people put data cleaning, think purpose class talk anything apply anything write apply models basic. Okay, so. So, want kind clean data, know stuff also really important. design features, get maximum amount useful information input data right, think features informative task hand. Right, one example gave classification dog versus cat know. Whether whether animal four legs feature officially extract going meaningless right going zero information doing. trying figure animal dog human, number legs, would really informative feature right so. keep mind, last sentence want know know, something i've received emails students working say computer vision projects, want to. machine learning projects want input certain things tell them, know I. work machine learning, area expertise text natural language processing right, cannot really. domain knowledge i've working images tell exactly futures might good mean, tell can. Talk general machine learning approaches done future engineering know futures whatnot help you. know, come models things like that, cannot. help comes processing raw data kind get meaningful set features get emails like time like domains knowledge in. domain knowledge, not, remember important it's. that's people specialize, natural language processing text added Professor Professor horizonte vision computer vision. so, even though we're machine learning lab Dartmouth, completely different domain expertise, domain, knowledge important, need get. field, need identify domain interested build domain knowledge. Okay, i'm going talk simple future engineering techniques know general, matter domain working with. One know programming features histogram features i'm saying obviously know whatever data working going hopefully numeric numeric map numeric data. one way extract informative features array information array data histogram data include like treat histograms features mean that. So. take look particular hypothetical data set case think it, medical data set your. consists patients vital measurements like blood pressure, blood glucose know blood type says input. listen writing working data, doctors blood pressure, blood glucose blood type input, could obviously passing raw data, know. input machine learning models, probably better process features little bit pass models so. look data say blood pressure glucose know numerical know use the. could use standard values given you, blood type medical right categorical one four categories right. features complete different types right, continuous features numerical could value categorical feature take four times right so. start start simple question is, we. represent this. feature blood type numerically. that, simplest way think representing blood type. yeah could assign different numbers block time would work. like without work say say always zero H1 b's ABS three would work. going come across lot map non numerical features numerical values. Alright, semi right answer, good approach mapping non numerical number. yes, why, people saying no, not. Okay, a. song, right answer, is. very, important, probably common mistake, see people feature engineering, first step feature engineering map non numerical data non numerical data right, obviously. important remember whatever models working mathematical models, meaning assign numbers saying ab three times a. Right, mean like mean B2B three times day makes meaningless right working mathematical space, point view models that's saying, wait. fine, course, like, weigh 100 pounds there's 300 pounds yeah three times much weight this, mean B2B three times. Right makes sense, want people graduate students I, makes everything invalid because. relationship meaningless right numerical relationships meaningless, yeah, is. Basically saying sorted like actually saying something distance saying points are. closer similar points away dissimilar whereas that's case right reason be. closer they're old right there's so, right representation anyone think alternate representation numerical representation. cheetah yeah other, you, map numbers, cannot cannot not. so. models work numbers, numerical representation way map non numerical categorical data numbers. So. Oh actually get forget part thing, summarizing saying actually put access it. arm making assumptions similarity idea day which, case make sense right, want avoid that. days distances, yes, could distance make sense, could i'm saying cases make sense, like case. Well, simple idea, good way encode categorical features word encode means turn numerical representation. use. histograms right. mean that. mean data applied corresponds one data point. instead one future blood type assigning instead one V churches blood type assigning label zero to. Three make sense, said, represent person's blood type collection histograms case going binaries one. zero person blood type O, blood type actually represented vector 10002 special blood type represented 0100 on, forth right, so, title histogram think given us. given a. hint we're trying so, way, differentiate different. Blood types without saying anything about. relationship variable either there, there. Right, makes sense right, person blood oh plots land types, say no, one numerical relationship. particular thing called using dummy variables sometimes people say well, might come across good to. called dummy variable basically taking one variable blood type turning four variables, so, fact, means adding adding futures your. data, instead one future blood time, futures, first feature second one a. Third, one B and, finally, B one yes zero very, important, is, always categorical features features correspond different categories right. Okay. sometimes called one hot encoding. called one hot encoding one turned case turn your. talk variables yeah take blood type tending blood dummy variables B. person going one turned called one hot encoding. process turning categorical categories dummy variables quite simple right removes issue discussed that's like simplest feature engineering, right found way deal categorical features. one important feature engineering, deal categorical features time. know, example, states someone leaves right make sense want include your. model make sense assign Massachusetts one new Hampshire new Hampshire somehow somehow twice State Massachusetts know make sense, so. 50 case dummy variables corresponding one us states one Turner. Okay, thing showed like simple category called histogram idea know, applies kind data look use histogram mean histogram features text data. So, similar concept the. Popular nlp natural language processing tools aware like sentiment analysis spam detection stuff. obviously based text data, know, things know text obviously America right so. need find way turn text vector veterans right, showed far previous example medical data blood pressure forgot glucose level, numerical anything made. Blood type categorical could easily turn know. Numerical features whatnot one hot encoding text data, would turn number first thought, way feature entering turn non. Numerical values numerical values, otherwise can't models, working ideas, vet tries document it. Think this. good answers yeah mean. The. idea discussed blood type APP applies text well, could actually use histogram longer going one hot encoding it's. Whereas person blood type right certain words use multiple times document going value one zero, could still histogram use histogram technique in. text pricing called reading bag words exactly sounds like word. one feature, we, blood type blood type made blood Type one feature right, one feature one futures, was. future always feature word future, instead one hot encoded count number times word appears document right, say yeah know pretty straightforward that's nice way counting the. number times certain words appear document, also means future vector going length English lexicon know hundreds maybe 100,000 maybe 50,000 right word English language gonna one feature. then, course, know processing reduce amount words looking so, example, could remove. Certain words know really informative called. Stop words right, goes back idea domain knowledge, work lot text data realize certain commonly use words like that, whatever meaning no. meaning, known information helps in, example, sentiment classification classification, whether use word. effect whether something positive negative. remove kind non distinctive words called stop stop words comes domain knowledge depends task. task figure whether sentence grammatically correct not, course word matters right word people use word. Okay, here's example say so, way text processing document corresponds data points data points call document. text processing, document could sentence, could whole article, could tweet tweet analysis so. two documents two data points say case analyzing sentences there's desire send working dogs word base cats doors, so, sentiment classification. words, highlighted words informative know they're basically told things shouldn't. effect and, fact, could even. Look realize they're informative seem used equally know negative positive sentences right, realize right seem any. correlation use words i'm something negative positive, could remove words they're called stop words. SS ids informative like there's something plural not. sentiment classification right, went process actually future engineering would look like text right, start document remove uninformative features. informative words know letters case SS be. move well, nlp basketball natural language processing called removing STOP words, words meaningless useful removing SS turning plural to. singular called lemme ization standing know that's that's standard future engineering text. detect document turns darkness CAD wars right good, reduce number features we're looking at, another document had. Another document said clear said cat. cat bad best grammatically correct say, someone said that. Then, removed yeses cat cats treated two different features right two distinct words so. make sense, two features, one cat cancer removing bye bye stemming make sure they're treated feature, Center classification, matter anyone think other. stemming limit ization removing. know, turning plurals singular things, know stemming could might helpful. say sentiment classification yeah removing n G right run running, example, cooking whatever, right now, mapped run right cook whatever matter. Removing tense word, exactly, know. know cooked warm cooking cook cook mapped cook move past tense move ED exactly. So, depending task course possible future engineering writer is. domain specific we're talking domain text also task specific talking sentiment classification sentiment classification. know, removing n G 10s 10s pull mapping close singular stuff makes sense, things might right. left two they're still numerical right, then, turn numerical values use histogram before. fit blood type, create a. word going treated single feature, actually whether exists not, CAP, happens know. they're appear possible sentence word dog twice reason right so, know account. course here, know, whereas blood type person could one blood type could actually multiple words present sentence right still data point turned vector deal with. done using histogram technique worry about. issue numerical values discussed earlier saying dog twice awards katie's anything like word treated separate feature. make sense done Okay, veteran run. know models fact this, could use your. know precept line logistic regression homework, create sentiment class right and. means collectively stuff sentences manually label positive negative right, labels them. training set, simple removing starboard memorizing get vectors pass vectors cause functions create second class for. there's question, dog use twice probably affect sentiment, see capture it, if, example, word best used. twice might effect sentiment yeah still captured account yeah learning part possible that. things matter used excuse, least know would enough, certain words, maybe number times. important, someone said movie really, really awesome really two times know, know might may informative, use once. Okay, simple future injury went raw text right feature vector model use introduced simple future engineering ideas, one obviously idea of. Houston grammy second one know, removing uninformative features case identified domain specific, case identified stop words. pull off. Okay, problem that, called bag words, basically document point data point basically think bag words just. scumbag word representation problems, think particular representation. Think Ukrainian sentiment classifier like machine learning intuitively creating sentiment class it, would you, give this, give this, able tell. Whether. Something know positive negative, there, mean might able add certain level but, instead, something seems missing here. whoa there's one. problem point side creates lot features that's true. know try that's true general, but, usually deal natural language processing, work English lexicon. try remove reduce number features, things like Star Wars limit ization try reduce that, issue. Yes, two days right context missing, words like best best best best right tell best appears right there's context captured all. know future engineering, careful things removing great language context super important right context missing completely and. way, even though context missing justice representation turns really good sentiment classification, mostly standard classification really simple task. lot tasks, know yeah context missing, so, need think carefully right, capture context, capture context. Right contacts word know. Well, taken account syntax maybe Okay, think, Sam Simon right, riches judah right answer keep track window, keep track neighboring words right, mean is. guess. change example reason that's answer correct, answer cheetah Simon Gary Craig want somehow window around words captures context which. they're used know word is, want somehow capture awards that's context work right and. well take look two examples, a, actually remember put this, actual data set reviews well known cagle tasks competition password give you. reviews, identify reviews positive negative and. two reviews there, histogram thing right we. Obviously, remove know stop words stamped right, see, like removed. removes Star Wars like removed things like that, removed words stamped like guaranteed guaranteed right like the. way, stemming sometimes generate something might might spelled spelling might correct, that's that's that's fine. So. Right, that, funny standing find forth alright, that, two parts reviews and. want right. want, feature future representation, whatever substantially came way, case, buy word histograms they're good want see slide like this, want. said, could toy. plug want, access, know. Words positive sentiments like good great whatever one side access, know words negative sentiment like bad know terrible. another side neutral words like like maybe know watch like movie review, like a. neutron word right, want, representation histogram good, want see kind shape where. want boards, negative sentiment, represented colors course one negative positive sentiment, want number reviews negative negative sentiment. represented side histogram the. Number reviews positive sent want represent side history right separation that's good your. histogram working well, means separating positive negative reviews and, way, mean like showing review, histogram me. know bloom came blue review negative blue means present blue review orange means yellow means was. represented yellow case, negative, positive. drinking two reviews right, actually kind looks kind flat, idea 1000 positive reviews apart thousand negative reviews keep playing histograms. thousand reviews average want look like something like this, want negative reviews man, awards negative sentiments and. positivity two more, one way see bag words representation discuss works well not. there's question. kind general question hard detect sarcasm text feels like positive sentiment, yes sarcasm detection super hard special task. requires much sophisticated features bang away absolutely. Okay, 550 think good good place start Monday i'm going continue idea, couple example i'm going build top this, terms start with. Simple kind idea histogram bag words representation. work well blood type blood types right, know procedure not, there's context black tie. presenter blood type there's, know language, could kind bag representation, misses context order words, basically, hearing. Next, what, want extend representation include that, machine, part feature engineering right we're trying figure best way represent texts data sentiment analysis. question future engineering material included final exam absolutely future Jenkins, important. i'm going ask, might ask questions, change name, course going included aside. mean obviously i'm going ask you, specific domains, knowledge saying, need domain expertise and, more. general questions feature engineering so, example, already know cannot convert categorical feature two numbers, know need use dummy variables one hot encoding histograms showed, already know right. that's something that's used future quite lot lot earlier categorical information time right block time one like said, state live in. College, went great say wanna come model predicts future income. Based characteristics like education whatnot say, one variables College, went. that, obviously, again, cannot know, dogma twice much College Harvard maybe know, know represented. Like one hot encoding right name colleges one call tonight categorical features everywhere knowing cannot represent single feature important. way, word warning, student. Dartmouth, undergraduate student working senior thesis and. Quite lady thesis realized clear make clear that. pretty much models presented whatnot one features actually categorical feature way assigned number two. Like zero 10 know, obviously, makes completely invalid results thought getting soon change one hot encoding collapsed. person finish thesis mean identified mistake way way late future. know mistakes happen quite lot, careful, always understand that. end day, working learning techniques talking working mathematical space mathematical space, numbers relationships there's meaning to. know closer three diabetes four right, so, represent data numbers, know aware relationships right every know we're running time, i'm going to. say one thing, wrap today, all. always wrong map kind features single number give case category actually map one single feature numerical future. Instead using know one hot encoding think one categorical feature like that. Well, one person said age kind age already continuous numbers so. BMI that's already kind number. simple say looking at. size clothing, Small Medium large extra large right categories, here, could actually one valuable says i'm going size zero size, want three relationship make sense large us closer extra large medium. versus small right yeah need use one hot encoding right that's remember relationship. values right, case one variable assign based blood type make sense, could closer right. color another potential favorite color someone said guess could be. True, yes color well. favorite color guess wait, guess, think terms actually. closes rgb value color action already represented usually numerically machine learning using rgb it's. Alright, great end lecture there's two lectures left i'll tell what. plan i've wrapped clustering lectures i'm going continue future engineering think good dig little deeper it. really want use methods real life, know actually carry decisions handed you. then. i'm going to. couple extra things people suggested cover last lecture i'm still deciding. might be, know i'm considering based feedback i've gotten lot people wanted maybe learn little bit like intro quick intro deep learning might people asked about. kind methods want go little deeper one person want give brief overview class teach data mining class teach could also would quick like five minutes overview, could also but. still ideas send need get ideas soon, I, even though lot slides software material complete new material might prepare new slides. send send me, think might something want learn think teaching alright, classes live quite bit time so. i'm going open questions go recording class related questions, questions we'll start back office hours, questions covered today. Okay, good evening i'll see Monday. 