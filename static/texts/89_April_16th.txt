 Too much information.
 sometimes gets in the way.
 i'm sure we all, you all know, someone who.
 is so worried about making the right decision that they end up never actually making a decision because they're so full full of concerning questions and.
 Just do it right.
 But that's what we're talking about today is decisions and judgment.
 it's been a pretty interesting book so far.
 really making clear that we we don't always know how we make our own decisions what what goes into those decisions when we do end up making them when do we make our best decisions.
 first thing in the morning.
 In the afternoon.
 let's see who is that that's Connor tell me when do you make your best decisions.
 Is there a good time in the shower.
 You know.
 I am amazed at how many times i've solved problems in the shower in you know not personal problems necessarily in math problems or whatever, as long as it comes to me why do you think that is.
 Any thoughts any reason why it's not like there's more data in the shower little go.
 you're not usually focused on anything else, particularly like consuming your mental energy in the shower so you can just kind of your mind will just kind of work on things.
 So you push it to the background.
 And what the background task goes i'm putting an ampersand on the end of it right, yes Tracy.
 I was gonna say like sometimes when i'm trying to solve a problem for like homework, or something like as soon as it gets hard like got like maybe a snack or like check my email or something, but then this Jerry they can't really do that.
 kind of forces you to like think about stuff.
 Okay, so you see it, the opposite way that the shower there's nothing else to do so, you tend to focus on okay it's going to be different for everyone kyle, what do you think.
 i'm kind of similar, but I think like in the shower it's sort of a it's a kind of relaxing like experience, so you I feel like a lot of times if you're under stress you tend to either like.
 Get like frozen on decisions or you overthink things but it's very like sort of a clean slate which is.
 kind of related to what was already said but.
 sort of the sort of the mental stress part of it, I think it's important to.
 know, especially the clean slate part.
 Okay, you did that on purpose right all right, Daniel.
 yeah I think sometimes our best like thoughts and decisions come and we're in a different state of mind.
 You know if you're in usual frame like sometimes it's kind of hard to step outside that but maybe when you're in the shower a lot of times you're alone and.
 You know, maybe you're thinking about something else, like when you're different state of mind, I think a lot of times like those creative juices can get blown.
 Okay yeah a colleague of mine insist that he writes his very best.
 documentation that is.
hammered.
 You know, beer and everything else, and you know, I have to admit it does seem to flow pretty well not to be the documentation so so when is it a bad decision is a bad time to make decisions okay so some would say first thing in the morning.
 Fine, pressure.
 Katrina.
 I don't think it's necessarily stress, because under stress I tend to make good decisions but it's specifically emotional stress like if i'm emotionally attached to the situation it's more difficult for me to not have a cloud in mind.
 Yes.
 Things things get in the way.
 Which of the twins do you have a space with this to the airport, the first time the twins were walking.
 They go to the airport it's cool and they run in separate directions.
 So what do you do.
 it's it's it was a the final answer was you go after the one who is more likely to get into trouble.
 it's pretty much easy to make that decision there's the curious one and then there's the crazy come in pairs like afraid of the outcome, no good options that's another one.
 We did it doesn't matter what I choose it's still gonna suck okay well that's that's a possibility for machines and our Ai helpers i'm not going to have this concern in the book he talked about.
 posing a question to doctors, whether they should have surgery, which has a 90% survival rate after six months or the same surgery, which has a 10% mortality rate in the first month.
 they're saying the same things.
 But they choose the other way, have any of you run into you know goofy percentages or other proofs that humans have, for the most part, just don't understand statistics.
 yeah I read or i've heard about one recently where.
 things on the go freakonomics podcast you're talking about how how much people be willing to pay.
 In order to avoid a certain risk of death, like.
 let's say you're going on a water slide.
 And I think if there was like a 1% chance of dying, people will be willing to pay 10% of their net worth which, like isn't entirely rational, but it just goes to show like we're not thinking properly statistically.
No.
 we're really not we're not good at all in the security course one of the things exercises we do is we go around the room and we try to pick random numbers and then we are real terrible at that, but Catarina what have you got.
 I think the whole coven crisis has shown that it's very interesting how people deal with numbers.
 Because when given the same statistic of how deadly coven was some people say oh it's like I forget the actual number but it's like only 1% of people die and so like.
 covert isn't really that big of a deal and other people really take it seriously and like that same number causes people to react differently.
 yeah yeah.
 I actually met someone this past week.
 We thinks it's all.
 A big.
 plot and he's not a trump Ian but you just think it's all a big plot to get us all to worry and I don't know what and then another guys told me yeah the chip problem we're all having it's you can't get enough chips to build radios for cars oh it's because all the chips in the vaccines.
 Okay.
 So there's times when we have bad information like that in times when we're here we just have to guess Irish you have something.
 Oh, it was just on the previous point, but if you've moved on.
 Since no no go ahead, please, no.
 It was something i'd read about it's not about humans not really understanding statistics, but how citizens can be used to like portray very bad situation so, for example.
 One thing that goes around a lot is that if women get pregnant after 40 then there's 100% increase in the chance that.
 i'm like there might be an issue with like the baby, but actually in reality that's just 2% from the original 1% before, so this is a marketing technique UCLA actually force women into not force but like.
 psychologically coax them in like getting pregnant early, which is again a simple explanation of how like humans just can't see statistics, the way that they should.
 yeah exactly and there's a great book how to lie with statistics.
 very, very popular Joseph.
 yeah even extending beyond statistics it's the whole concept of relative placement of numbers.
 comments of sq they did the experiment where if you're gay people one times two times three times four times five times six times seven times eight.
 Their annual actually be less than someone who guesses eight times seven times six times five so on until one, even though it's the same answer you'll see the predictions are lower.
 For the increasing sequence yeah it's fun to mess, with people.
 And I guess that's what mark advertising and marketing is all about.
 So work we're talking about those we're trying to give.
 These prediction machines, a better.
 Better a way to do better than we do.
 And as the book pointed out, sometimes we're not going to be necessarily happy with that or we're not going to be not sure we're going to agree, but take the example from the baseball example.
 We have all these these recruiters who have been spending years coming up with their own little style of anybody from this high school that played in this position, and my favorite has a good looking.
 Life partner.
 Great we choose them because they come from the right school and they have a good looking life partner come on.
 But there must have been some data that supported that at some point.
 At least in their small sample so a lot of times, we will be giving these things, these systems data that well it's data, we would use.
 Maybe that's not such the right but not such a good idea of this so anyway.
 we're talking about the decisions and the decision machines i'm sorry if the prediction machines we don't have decision machines yet if the prediction machines.
 are doing okay.
 Then we have to decide whether we want to believe what they tell us.
 And so that's that's another whole tricky issue that will we'll talk about the moment.
 let's let's touch a little bit on Mr weinberger is weinberg, no, no, I was Rumsfeld guests, yes, I actually have shared a note elevator with Mr Rumsfeld once at the Pentagon, and he always looks cranky Iraqis, if you don't know who that was he was the.
 Secretary of Defense a few years ago we've just always cranky.
 And he's taller than he looks on TV, but he made a very widely reported speech that's talked about in the book.
 About stuff we know there are things we know that we know.
 There are things we know we don't know so I know that there are 32 people in the meeting today counting myself I don't know how many of them are awake okay.
 There are unknown unknowns this is stuff we don't know we don't know so.
 I don't know that those who look like they're asleep are actually having wi fi troubles.
 But he stopped there, and of course the book went on to talk about the unknown knowns, which is when something we thought was a really good predictor.
 turned out the longer or not always be such a good predictor does anybody had any experience of that something some data that you depended on.
 And then, all of a sudden, the data is not accurate anymore, or are you realize that it's been variable all this time, and maybe some of your decisions weren't so great, after all.
 About what's that course think a layup.
 The ever i'm sure no one would admit to it, but there might be some of you who looked at layup for taking a course took the course and then found out, it was a bear.
 That the Professor found out what people were saying about the course and made some changes that could happen Joseph What about you.
 yeah this is definitely happened to me.
 um any example you can share.
 wow I took a you know, a public policy course and I didn't I didn't know what was going on but.
 I thought it was just going to be a regular course because that is what I like online, but it had switched the Prophet I hadn't known it and the and the prophets like okay so we're going to have like you know 30 different assignments, a week and i'm like oh i'm in trouble.
 yeah.
 yeah anybody else.
 How about hiring.
 yeah I actually I have an example.
 Yes, please.
 um there's this new movie or a pretty new movie called see spirits he's like a documentary and they put up a lot of like numbers about you know ocean fishing.
 And that sort of thing and then like recently it's come out that, like a lot of their numbers, they have like numbers about.
 bycatch and like stuff like that, but a lot of their numbers are actually old so they have like higher numbers and they kind of like it use that to exaggerate so something like that.
 yeah yeah anything that.
 appeared to good or a good correlation and good correlating element in the past, now all the sudden.
 you realize either the data was crap or the data was manufactured or the data is not stable yeah so.
 This is a this is kind of a challenge, but we all face this and we kind of somehow we learn well, some of us.
 learn to be able to identify.
 Bad data.
 To be able to identify the things that we know and the things that we don't know.
 How do we do that, how do we learn.
 To tell the difference between good data and and bogus data, how do we, how do we do that.
 Just trial and error.
 tanya.
 Like in some circumstances you're kind of taught that like.
 I know, like in grade school like we would always go to the library, and they would show us like how you can like tips for telling a website is a good website to site in like a paper.
 or thinking about like what information is on the screen kind of in that using that sort of as An informant for whether or not like the source is verifiable so, in some cases you're taught.
 yeah okay.
well.
 So I think this this example that I have is not necessarily people interpreting the data incorrectly, but rather a statistical phenomenon that kind of represents people not really understanding the concept of average when surveyed I think like 90% of people will.
 will answer they believe that they are an above average driver, which clearly not the case.
 yep yep very true.
 very true.
 So Okay, so this, this is all interesting stuff and it's it's fun to look at decisions and try to see all right, I made that decision.
 And then revisit it okay Why did I make that decision and that's what we're having to do with our prediction machines.
 Is we want to trust them, we want to provide them with what we hope is unbiased data and we're going to have two speakers on that.
 But we don't always we don't always have unbiased data so that's that's another challenge we have, so we need to test our prediction machines, to make sure that they work out right.
 I guess stockbrokers are always doing this kind of thing right is the stock going to go up is it going to go down, and so on.
 alright.
 So let's.
 make a very strong point about how we can use prediction machines, to make a prediction, but they differentiate between prediction and judgment.
 So.
 How does anybody have an example, maybe your own decisions or whatever, where you've you've been given a prediction, and then you make a judgment as to whether or not believe it that's essentially what we've been saying.
 Like I was at the car dealer today, and he said, you know your brakes the brakes are down to two miles, you really ought to do something.
 So he's predicting.
 That i'm going to have a break trouble.
 Coming up.
 Okay he's not saying how long it'll be he's not providing input about how long it's been since the brakes were at their full 12 miles.
 But he's telling me you really ought to change his predicting that a couple of things that i'll have trouble in that i'll go along with it and let him do the $400 job.
 I have to use judgment.
 Based on things I know.
 These are reputable reputable people.
 And while I don't know the service person in general that's an unknown.
 He or she is associated with unknown unknown for believed to be known good quantity okay.
 Now unknown knowns unknown unknowns um.
 I don't know if they're getting a price break from breaks or us or something that's encouraging them to do break john's.
 But who knows, so I have to make a judgment, given all that stuff and then off I go and that's kind of a particularly vexing problem because.
 i'm not back there with the with the measuring device, so I don't know, I have to go based on this this person's prediction so that's that's an application of judgment and.
 incomplete information so tricky say under.
 yeah so like, for example, once I was trying to book the hotel to dragon deafness and some top recommendation that comes to me like booking this hotel is better than this one, but.
 that's actually based on some previous data or feedback from the customers of the previous time, but this time I see that there is some different features like i'm not going to stay for like.
 Only one week i'm going to sit there like two weeks, and there are some other friends with me traveling so I need to make.
 think carefully make which hotel am going to be people on that the one that has the highest price or that is the luxurious or the one that is lowest price spiny to.
 use my own thinking to make the Jasmine like who each combination like I might choose like staying five days in one hotel and rest of the days in another hotel or doing something minimizing my costs and enjoying everything So those are things that come to my mind.
 yeah that's an example again like from the book where you're using information that's not available to the system that is predicting a good choice right don't you.
 This is sort of similar to your example Professor but i'm like my computer oftentimes like the battery will say like service battery or like you need to fix your computer battery.
 But i'm making the judgment to ignore it, because it still works like if I plug it in to the outlet, and I know like it's going to cost a lot of or like a decent amount of money to fix it so for me like making that cost benefit analysis it's like not worth it to fix it yeah.
 i've got a laptop like that.
 No.
 Charles.
 yeah this is maybe a little simple but I guess for the weather, today, you know we all got the prediction that there's like 100% chances now.
 or just I guess from personal experience I sort of doubted it and even though sort of the prediction was an accurate, in the end, I question whether would actually snow today just do the weather that had come before.
 That we all did I actually.
 canceled my my.
 auto repair or point thinking it was going to be World War three or something nope nothing.
 So.
 Mr couple.
 um so i'm graduating senior i'm also international so I have to apply for a work visa, without which I can leave the country and get back in.
 So, right now, my already planned post graduation essentially change army predicting correctly by what did I received my visa.
 So i've already started to like look at flights to go back home and stuff, but I can I have decided to make the decision of either willing to last minute to book flights or booking a flight now and hoping that my prediction of what I got my visa back is correct.
 Yes, yes, we have the challenge at IBM as well, and I believe your short name is abby is it.
 Okay, good just want to make sure you saw you on linkedin oh cool okay don't you.
 yeah I think another point that I use a lot of predictions is I do a lot of research before I go out to eat or try new restaurants, I look at a lot of yelp reviews.
 or Google reviews and like I feel like I take that into account, but sometimes they're like ratings are always accurate to me.
 Sometimes the readings, or you know skewed they even if the ratings bad, but like I know the restaurant and it's to my taste i'll still go or if our restaurants are rated really good that i've heard like bad things that word of mouth right not go.
 yeah and things little other things that an Ai or restaurant bought might not know is or might not have programmed into it as like don't go to associate restaurant on Monday.
 You know odds are the fish is not as perky as it would have been long maybe Friday so little subtleties like that, if we have the opportunity to add our additional information.
 Maybe that's okay.
 Stewart.
 Yes, an interesting example I thought of was where prediction comes into play is in search and rescue if someone goes missing there's a whole portfolio, on which you know which way someone would go if they're lost so like you know, for example.
 You know if you have dementia there's there's like one path it's it's most likely and if there's if you have autism there's one path and there's and then you set the.
 The search zones, based on these like predictive models and there was like an interesting example I know of where there is a there's like an eight year old boy with autism, who went missing.
 And they based on the model it predicted that this like specific scenario would often lead to like that kid hiding in over to her in.
 So generally right you take a path you you either go out the woods, or you walk or you stay in one spot and in this like class it's often found that.
 kids will hide, especially in like under logs or an overturned trees and based on that model that the search numbers found him.
 In an overturned system, which was pretty interesting and so based on that model you're able to like get the highest probability of of which direction will go or where they might be hiding.
 You know that i'd never heard that that's really, really good to think about because in new Hampshire just last week we had somebody.
 who had hiked up the side of Washington Washington with the skis can ski down he didn't quite make it to the top and but he did find his way to the bottom.
 Quite a quite a mess and hikers there's always hikers who you know who would who would go out on a hike with their cell phone at 10% come on.
 So there's a movie that came out when I was a kid some you may not have seen it 2001 a space odyssey policies has anybody seen that and Stanley kubrick okay well.
 it's a classic SCI fi thing and kubrick, of course, is amazing, the Director and an Arthur C Clarke book.
 The scenario, there is a super Ai.
 That is managing and running the ship on the way to Jupiter.
 And it's motivated by.
 Some signals or some some scary interesting thing that's out there okay part of the way, there are the astronauts began to question.
 The.
 accuracy of some of the predictions that the Ai was make.
 And because the Ai is really smart.
 They decided to go inside one of the escape pods to have a private conversation, because they have control over that environment and they can talk freely and, of course, the computer, how was reading their lips, through the window.
 Of the escape pod so them and one of them goes outside to retrieve the other who have had an unfortunate accident how refuses to reopen the pain, the bar the pod Bay doors.
 Because he doesn't want them to interfere with the mission.
 So here we have an agi.
 That is predicted.
 That the humans are going to turn it off.
 And, as a result, it will not be able to fulfill its mission.
 Was this the right angle to take mission is job one don't let the people die but mission is job one.
 will see again later in the term.
 we'll revisit this but was the computer how justified in doing what it did so now, I can let you back and then you're going to abort the project and I got to get this done, how do we avoid that.
 wow what he knows.
 But we have a guess, we have to.
 buy us these decisions and.
 it's not job one it's just.
 Job hi, so to speak, and can we build a cognitive system that's.
 going to avoid these mission this over focus on the mission, yes, Carl.
 um yeah I mean it's kind of similar to what the book was talking about with like reward function engineering, but instead of like having a strict like.
 algorithmic hierarchy you basically can just sort of wait outcomes based on so instead of job, one being keep the humans alive job keeping humans live is a very important and very high highly weighted.
 goal, but one other costs like come up, you can you sort of associated negative costs that, at a certain degree, will overpower that objective so it's kind of just a combination of these different board functions, even if in the end the human humans alive, is very important.
 Yes, very good and to follow on that one analysis that I had voted was.
 Maybe the humans weren't really necessary.
 Maybe how i've been.
 program taught trained that yeah it's important to be really good press but.
 If one of them goes loony or good sick or something it's it's no big deal so that was one of those you think it's operating under within certain parameters when actually you're not aware of what's really.
 anything's possible don't y'all.
 I mean there's a lot to like of considerations I go down to how to really define ethics of combination robotics.
 But I think most computation today follow the what is that, as a mom not sure how you pronounce that, but the three rules of robotics where it's like when you can't hurt humans to you have to obey humans.
 You know, considering as long as you don't violate the first rule and then three was like then fulfill the mission and protect itself it doesn't conflict with the first two.
 So I think like in your example of the space odyssey.
 If, like I don't know that would count as hurting humans by not saving them or not obeying them as a second rule.
 yeah I mean that's that's what I figured would come up as someone's encountered the three rules of robotics from Dr as a mom.
 And they.
 They pretty much stand the test of time and that book is really oh that's what's older than me wow but he really thought them out pretty well now can we could we implement that that's a whole nother thing you want the machine have enough freedom to explore the unknown unknowns.
 But.
 yeah, how do you put alicia yeah very tricky.
 Okay, so one of our goals is to.
 to recognize that we can have all the best data in the world and everything but the api's that are training are going to learn.
 they're going to learn things that we might not know.
 evolves and I don't have any examples, unfortunately, but we've all seen in the news people saying Oh well, the Ai was trained to recognize a stop sign, but if I put these two or three little marks on the STOP sign.
 You can't it doesn't recognize it anymore.
 that's a little bit it's subtle, but the reason that happens is that we don't really know what the Ai is using to make those decisions.
 We don't know.
 Whether it's using the shape of the sign the color of this is what data points are there, going into making that decision, what data points are there that we can mess with.
 That will make it recognize a cat as a dog or a tree and it's sadly not that hard so that's part of the challenge as well as we can provide all the data we want.
 But we what we don't always know is how that data is being used think back to our neural network.
 If you've got a 10s of thousands node deep learning neural network.
 All those weights in there.
 you'd have to know all the weights you'd have to know what each little note is looking at in quotes.
 and try to make sense.
 and
 Early neural networks, we could do that not anymore so.
 Treating.
 Okay.
 Can you think, and you know we are going to finish early today can you think about a situation.
 Where the number of possible judgments is sufficiently.
 finite that really could give the decision to a machine i'll get to you, Joseph.
 So some kind of flow of data, providing inputs to your Ai system and.
 The number of judgments.
 Possible judgments are it's it's fixed.
 Now does anybody have cruise control.
 You might say, well that's not an Ai will think about it it's getting continuous input from this speed sensors on wheels.
 it's getting continuous input from the brake pedal.
 it's not really learning anything but it's sort of.
 machine making judgment Oh, and now you can add the radar and the front of the car making judgment.
 You to close on so you don't of course tesla.
 It does what it does pretty well I don't know if it doesn't the learning based on your style of driving but I guess it doesn't need to because it's driving for you nevermind Joseph please.
 And this is like at the last point about like we don't know like what the weights are necessarily and I think the famous example is of.
 An Ai was given pictures of wolves and dogs try to recognize between the two, and it was doing it pretty well, but then, when they tried to test it, they give it like.
 a picture of a wolf and it said it was a dog and failed like horribly and they're wondering like, why is it that you know.
 Sometimes it's really good at making predictions and sometimes it isn't because they went, and they actually tried to figure out like what it had learned.
 And it turns out that what I had learned was to adore the dog or the wolf entirely and look at the outside, because all the pictures of the dogs are inside of like a building.
 And all the walls were outside in nature it's.
 Like well that's not really all that helpful, have you know learned behavior.
 Right right.
 And we would encounter this every year when we did.
 what used to be lab one, which was the visual recognition.
 People I would tell people look when you have your training pictures make sure they're all you know similar backgrounds similar head on shots or side shots just be consistent.
 And that's exactly the example that you gave that if you're not consistent and it's even more likely that Ai is going to detect something that you didn't think about and make decisions on that.
 junior.
 there's a fun field called like adversarial machine learning so you're basically trying to come up with ways in order to trick machine learning samples and it's like a security hole.
 kind of field, and one in one paper they actually found that there is a way that if you.
 manipulate just one pixel like a particular place of the image they could consistently trick the classifier because of the way it's trained so that obviously would be pretty imperceptible to the human eye, but because of the way machine learning models are trained.
 that this could cause.
 A lot of air overall.
 Absolutely, now that was a good if it's the paper and thinking it goes a pretty good story.
 um yeah and and how do we avoid those kinds of things I guess learning about the mail, which is why adversarial machine learning or adversarial.
 suit the generative adversarial networks gans yeah generative.
 adversarial networks, basically, you have to prediction machines training each other.
 One says, well, I think this is a dog, though, it sounds like no that's not much of a dog here's a dog.
 And it just goes much, much, much faster and it's scary how effective they are, and of course there are key to a new phenomenon called.
 Deep fakes.
 Where you're hearing past President Obama saying things that you're pretty sure he never said.
 But there you go conflict of interest can our machines have conflicts of interest.
 it's all about the data.
 Your your loan application bought.
 It only knows what you told it.
 It may, as as Daniel pointed out, it may make inferences that you can't see.
 It may end up making inferences based not on income and credit rating and my ends up making inferences on address.
 or area code or zip code.
 You don't know it's all part of the data that you throw into the hopper, and so we have to be extraordinarily careful to make sure to avoid the bias, even if it's a bias.
 That we ourselves admit to having that would be careful.
 Okay, so.
 Building these systems that we can trust.
 it's kind of a big deal because we're trusting them to give us the predictions, and then we have to do the judgment thing we have to decide what is the payoff function of that what is the reward function.
 In Daniel Danielle picks a good restaurant the reward function is she's she enjoys her meal.
 Otherwise, she has to buy tom's okay that sort of.
 reward functions on hiring book talks about it, people don't quit.
 We needed this that we have because we hired a guy the way from a let's just say large West Coast software company.
 And he shows up and.
 We have fantastic skills we had all kinds of things going for him and he shows up we're so happy to have him and he says so where's the food and we said incidence of cafeterias down the road there.
 He goes away and it comes back and so it's not a frame and said well duh.
 He ended up quitting.
 And he says i'm not sure if it's true, but he says it's because he was used to free food and he was one of those people that will work 20 hours a day, and he said I just need to have access to food and.
 You know, so what.
 Other prediction holes and employment another example.
 we've hired a guy who was brilliant glowing reviews all kinds of good stuff that is bad.
 It nailed all of our measurement points we, this is not even an ios was just us.
 And he shows up.
 In stinks I don't mean older and he's a horrible program and he doesn't show up and and and.
 So we called his previous employer.
 But we'd like to talk about Michael Oh, the guy says what about him.
 And we start asking questions and he says wait wait wait wait, the guy on the phones as you're talking like Michael is there he's here i'm looking at it, what do you mean he's not at work.
 Michael has point.
 identical.
 Who was a bit of a slacker.
 The productive brother would get the job.
 And the slacker would show up and take it until he got fired.
 or algorithm didn't take that into consideration that.
 Who would have thought.
 Oh, and they're both were also illegal at the time but anyway that's another discussion.
 Who would have thought that was part.
 Of the algorithm to make sure that the person that fits your goal here is actually the person who's going to prepare.
 So is it identifying fraud yeah that's that's the challenge we didn't meet that time.
 Any other thoughts.
 Determining the payoff is challenging.
 You think it's it's to have a good meal, but.
 Sometimes you're going to choose a restaurant, based on the ambience foods crappy and expensive, but my significant other likes.
 So, so the the judgment, the reward function may vary.
 Based upon the newlyweds you got one kind of reward function people been married 50 years yeah there's a very different reward function.
 So anyway well that's about all I have today so.
 I hope you're having good progress on the assignment.
 I haven't noticed any.
 Loud screams of pain from slack so i'm assuming things are going well, if you do have questions and i'll hang around to the end of class today and also our graduate ta has offered office hours i'm going to put some up as well i've just been.
 neglectful there, so any questions on today's content, if not.
 The please hang around if you have questions about the homework and otherwise i'll wish you a happy weekend and see you on Monday.